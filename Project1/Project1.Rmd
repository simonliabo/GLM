---
title: "Project 1"
author: "Erlees@stud.ntnu.no (10028), Simonli@stud.ntnu.no (10014)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
```

# Problem 1

### a)

test

The pdf for a Poisson distribution is $f(x)=\frac{e^{-\lambda_i}\lambda_i^{y_i}}{y_i!}$ where the expected value $\lambda$ is given by the cononical link function $\lambda_i=e^{\eta_i}$ and opposite $\eta_i = \ln(\lambda_i)$. To find the log likelihood function we first need the likelihood function $L(\beta)$. $$L(\beta)=\prod_{i=1}^n{\frac{e^{-\lambda_i}\lambda_i^{y_i}}{y_i!}}.$$ Fahrmeir (2013, p.654) says the factors $\frac{1}{y_i!}$ can be omitted when maximizing the likelihood because they do not depend on $\lambda$. I therefore exclude these factors when deriving the expressions.

Log likelihood function:

\begin{equation}
\begin{split}
  l(\beta) & = \ln(\prod_{i=1}^n{e^{-\lambda_i}\lambda_i^{y_i}}) \\
  & = \sum_{i=1}^n{\ln(e^{-\lambda_i}\lambda_i^{y_i})} \\
  & = \sum_{i=1}^n{\ln e^{-\lambda_i}+\ln\lambda_i^{y_i}} \\
  & = \sum_{i=1}^n{-\lambda_i+y_i\ln\lambda_i}
\end{split}
\end{equation}

Fisher score function:

\begin{equation}
\begin{split}
  s(\beta) & = \frac{\partial l}{\partial \beta} \\
  & = \frac{\partial}{\partial \beta} (\sum_{i=1}^n{-\lambda_i+y_ix_i^T\beta}) \\
  & = \sum_{i=1}^n\frac{\partial}{\partial \beta}({-\lambda_i+y_ix_i^T\beta}) \\
  & = \sum_{i=1}^n{-x_i\lambda_i+y_ix_i} \\
  & = \sum_{i=1}^nx_i(y_i-\lambda_i)
\end{split}
\end{equation}



Observed Fisher information:

\begin{equation}
\begin{split}
  H(\beta) & = -\frac{\partial^2l(\beta)}{\partial\beta\partial\beta^T} \\
  & = - \frac{\partial s(\beta)}{\partial \beta^T} \\
  & = \frac{\partial}{\partial \beta^T}(\sum_{i=1}^nx_i(\lambda_i-y_i) \\
  & = \sum_{i=1}^n\frac{\partial}{\partial \beta^T}(x_i(y_i-\lambda_i)) \\
  & = \sum_{i=1}^nx_i\frac{\partial\lambda_i}{\partial\beta^T} \\
  & = \sum_{i=1}^nx_i\frac{\partial \lambda_i \eta_i}{\partial\eta_i}\cdot \frac{\partial \eta_i}{\partial\beta^T} \\
  & = \sum_{i=1}^nx_i\lambda_ix_i^T\\
  & = X^TWX
\end{split}
\end{equation}


Expected Fisher information:

\begin{equation}
\begin{split}
  F(\beta) & = \sum_{i=1}^n E[s_i(\beta)\cdot s_i^T(\beta)] \\
  & = \sum_{i=1}^n E[(y_i-\lambda_i)x_i\cdot(y_i-\lambda_i)x_i^T)] \\
  & = \sum_{i=1}^n E[x_i x_i^T(y_i-\lambda_i)^2] \\
  & = \sum_{i=1}^n x_ix_i^TE[(y_i-\lambda_i)^2] \\
  & = \sum_{i=1}^n x_ix_i^T\cdot Var y_i \\
  & = \sum_{i=1}^n x_ix_i^T \lambda_i \\
  & = X^TWX
\end{split}
\end{equation}

In the equations for observed and expected Fisher information, we use a matrix $W=\text{diag}(\lambda_1,\lambda_2,\dots,\lambda_n)$. We observe that the expected Fisher information is equal to the observed Fisher information. This is because the observed Fisher information do not depend on $y_i$, and the logit is the cononical link function.

\newpage

### b)

```{r}
score = function(y, X, beta) 
  {
  eta = as.vector(X %*% beta)
  lmdba = exp(eta)
  score = apply((y - lmdba) * X,2,sum)
  score
}
expected_fisher = function(X, beta) 
  {
  eta = as.vector(X %*% beta)
  W = diag(exp(eta))
  t(X) %*% W %*% X
}
log_likelihood = function(y, X, beta, lmbda = exp(as.vector(X %*% beta))) 
  {
  sum(ifelse(lmbda==0,0,y*log(lmbda) ) - lmbda)
}
myglm = function(formula, data, start=rep(0, ncol(model.matrix(formula, data)))) 
  {
  X = model.matrix(formula, data)
  response = as.character(formula)[2]
  y = data[[response]]
  beta = start
  
  s=1
  while (s > (1e-10)) {
    eta = as.vector(X %*% beta)
    lmbda = exp(eta)
    
    score_val = score(y, X, beta)
    f = expected_fisher(X, beta)
    
    beta = beta + solve(f) %*% score_val
    s = sum(score_val^2)
    }
  #vcov
  cov_mat = solve(f)
  
  #coefficients
  sd_err = sqrt(diag(cov_mat))
  coeff = cbind(beta, sd_err)
  colnames(coeff) = c("Estimate", "Std.Error")
  rownames(coeff) = paste0("beta_", seq_along(beta)-1)
  
  #deviance
  dev = 2 * (log_likelihood(y, X, beta, lmbda = y) - log_likelihood(y, X, beta))
  
  list(coefficients = coeff, deviance = dev, vcov = cov_mat)
}
```

### c)
```{r}
n = 1000
k = 2
#simulate data
beta = rnorm(k+1)
X = cbind(matrix(1,n),matrix(rnorm(n * k), nrow = n, ncol = k))
eta = as.vector(X %*% beta)
lmd = exp(eta)
y = rpois(n,lmd)
data_sim = as.data.frame(cbind(y,X[,2:3]))
#fit models
model_myglm = myglm(y~., data = data_sim)
model_glm = glm(y~., data = data_sim, family = poisson(link=log))
#evaluate
coeff_diff = mean((model_myglm$coefficients[,1] -model_glm$coefficients)^2)
coeff_diff
vcov_diff = mean( (model_myglm$vcov - vcov(model_glm))^2)
vcov_diff
```
The model looks good. The results are very close to those obtained with glm() and vcov().


# Problem 2
```{r task2}
load(url("https://www.math.ntnu.no/emner/TMA4315/2022h/hoge-veluwe.Rdata"))
```

In problem 2 we will consider a data frame containing data on the bird Great tit in the national park of Hoge Veluwe. The data was collected on 135 female birds in the summer of 2005. 

The response variable is the number of fledglings leaving the nest, which relies on the time of initiate breeding and the number of fledglings for each bird, plus the timing of food resources. The number of fledglings follow a poisson distribution with expectation $\lambda_i(t_i)$. This dependence is explained by a gaussian function $$\lambda_0\exp(-\frac{(t_i-\theta)^2}{2\omega^2})$$.


### a)
In the gaussian distribution defined by the expression above 
$\lambda_0$ is the highest number of expected fledglings, or $\lambda_i$ at $t_i=\theta$.
$\Theta$ is the time at which the expected number of fledglings is the highest, the optimal time.
$\omega$ is the variance of the expected number of fledglings.

### b)

A generalized linear model needs a random component, a systematic component and a link function which can give the relations between the GLM parameters contained in $\beta$ and $(\lambda_0,\theta,\omega)$.

In this situation the random component is $y_i$ which as said follows a poisson distribution. The systematic component is $\eta_i=t_i^T\beta$, and the relation can be explained by $\eta_i=\log(\lambda_i)$ which is a canonical link function.

The link function gives $$\eta_i = \beta_0+\beta_1t_i+\beta_2t_i^2=\log(\lambda_i) = \log(\lambda_0+\exp(-\frac{(t_i-\theta)^2}{2\omega^2})) = \log(\lambda_0)-\frac{(t_i-\theta)^2}{2\omega^2} $$

Hence, the relation between $\beta$ and $(\lambda_0,\theta,\omega)$ is

\begin{equation}
\begin{split}
  \beta_0+\beta_1t_i+\beta_2t_i^2 &= \log(\lambda_0)-\frac{(t_i-\theta)^2}{2\omega^2}\\
  & = \log(\lambda_0)-\frac{1}{2\omega^2}(t_i^2-2t_i\theta+\theta^2)\\
  & = (\log(\lambda_0)-\frac{\theta^2}{2\omega^2})+(\frac{\theta}{\omega^2})t_i+(-\frac{1}{2\omega^2})t_i^2
\end{split}
\end{equation}

### c)
```{r task2c}
m.birds <- myglm(y~t+I(t^2), data=data)
m.birds
```

### d)

To check if the data provides evidence of a quadratic effect of $t$ we perform a hypothesis test, with $$H_0: \beta_2 = 0; H_1: \beta_2 \neq 0$$. We then fit two models; one with the quadratic effect of $t$ and one without. We know the deviance for both models, and that, under the null hypothesis, the difference between the deviance is asymptoticly chi-squared distributed with one degree of freedom. 

```{r task2d}
m1 <- m.birds
m2 <- myglm(y~t, data=data)

dev1 <- deviance(m1)
dev2 <- deviance(m2)
devdiff <- dev2-dev1

p_val <- 1-pchisq(devdiff, 1)
p_val
```

As the p-value is `r p_val`, which is less than the significance level 0.05, we can reject the null hypothesis. That is, there is evidence that there is a quadratic effect of $t$ in the data.

### e)
For sufficiently large number of observations, $n$, the deviance, $D$, is approximately $\chi_{n-p}^2-$distributed. We do a hypothesis test with $H_0$: The model is a good fit, and $H_1$: The model fit is bad.

```{r}
n = length(data$t)
p = nrow(m.birds$coefficients)
df = n-p
D = m.birds$deviance

p_value = 1 - pchisq(D, df)
p_value
```

The p-value is smaller than any significance level we might want to choose. We therefore reject $H_0$ and conclude that the model is not a good fit.

We take a closer look at our data to examine why our model is not satisfying. 

The plots below show the data in a few different ways: y against t, y against the expected value (from the fitted model), and the residuals against the expected value. For all of the plots the black line is the expected value of y, and the dotted blue lines show the variance.

```{r}
#calculating expected values, lambda, and residuals, based on the model fit.
beta = m.birds$coefficients[,1]
lmda = exp(beta[1])*exp(beta[2]*data$t)*exp(beta[3]*(data$t)^2)
E_y = lmda
data = cbind(data, E_y)
names(data)[3] = "E_y"
res = data$y-data$E_y
data = cbind(data, res)
names(data)[4] = "res"
```

```{r, echo=FALSE}
p.res = ggplot(data, aes(x=t)) +
  geom_point(aes(y=y, color=abs(res))) +
  scale_color_continuous(low = "black", high = "red") +
  guides(color = "none") +
  geom_line(aes(y=E_y)) +
  geom_line(aes(y=E_y+E_y), color="blue", linetype="dashed") +
  geom_line(aes(y=E_y-E_y), color="blue", linetype="dashed") 
  #geom_segment(aes(y = y,xend = t, yend = lmda), alpha = .2)


p.res_2 = ggplot(data, aes(x=E_y)) +
  geom_point(aes(y=y, color=abs(res))) +
  scale_color_continuous(low = "black", high = "red") +
  guides(color = "none") +
  geom_line(aes(y=E_y)) +
  geom_line(aes(y=E_y+E_y), color="blue", linetype="dashed") +
  geom_line(aes(y=E_y-E_y), color="blue", linetype="dashed")


p.res_3 = ggplot(data, aes(x=E_y)) +
  geom_point(aes(y=res, color=abs(res))) +
  scale_color_continuous(low = "black", high = "red") +
  guides(color = "none") +
  geom_line(aes(y=0), color="black") +
  geom_line(aes(y=E_y), color="blue", linetype="dashed") +
  geom_line(aes(y=-E_y), color="blue", linetype="dashed") 


grid.arrange(p.res,p.res_2,p.res_3,
  widths = c(1, 1, 1, 1),
  layout_matrix = rbind(c(1, 1, 2, 2),
                        c(NA, 3, 3, NA))
)
```

We observe, especially from the third plot, that the variance assumption seems fine. The residuals "fan out" for larger expected values as expected. The data might vary a bit more than expected, but lack of an over-dispersion parameter is probably not our main problem here. 

We see that the data, y, tend to miss more frequently above what the model predicts than below. y is also 0 more frequently than we would expect from the model, both when the model predict low and high values.

We make another plot to get a better idea of what might be wrong. We simulate data from the model, using bootstrapping to sample t, and plot density plots of the data and simulated data. Red is the actual data, blue is the simulated data, and the vertical, dotted lines are the means.

```{r}
#simulate data from bootstrap sample of t
n = 1000
t_sim = sample(data$t, n, replace=TRUE)
lmdbd_sim = exp(beta[1])*exp(beta[2]*t_sim)*exp(beta[3]*(t_sim)^2)
y_sim = rpois(n,lmdbd_sim)
d.sim = as.data.frame(cbind(y_sim,t_sim))
```

```{r, echo=FALSE}
p = ggplot(data, aes(x=y)) +
  geom_histogram(aes(y=..density..),binwidth = 1, color="black", fill="red", alpha=.2) +
  geom_histogram(data=d.sim, aes(x=y_sim, y=..density..),binwidth = 1, color="black", fill="blue", alpha=.2) +
  geom_vline(aes(xintercept=mean(y)), color="red", linetype="dashed", size = 1) +
  geom_vline(data=d.sim,aes(xintercept=mean(y_sim)), color="blue", linetype="dashed", size = 1) +
  geom_density(color="red", size = 1) +
  geom_density(data=d.sim, aes(x=y_sim), color="blue", size = 1)
p
```

Again we see that the actual data is 0 way more frequently than the model expects. Our model skews too much to the left to line up the means. Except for this peak at 0 however, the assumption of a poisson-distribution does not look too bad.

We speculate that the process of "whether any fledglings leave the nest" (binary with y=0 or y>0) and the process of "how many fledgelings leave the nest" might be better modeled separately.

### f)


\begin{equation}
\begin{split}
  \hat\beta_0 &= \log(\lambda_0) -\frac{\hat\theta^2}{2\hat\omega^2}\\ 
  \hat\beta_1 &= \frac{\hat\theta}{\hat\omega^2}\\ 
  \hat\beta_2 &= -\frac{1}{2\hat\omega^2}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
  \hat\omega &= \sqrt{\frac{1}{-2\hat{\beta_2}}} = g(\hat{\beta_1},\hat{\beta_2}) \\
  \hat\theta &= \omega^2\hat{\beta_1} = -\frac{\hat{\beta_1}}{2\hat{\beta_2}}  = h(\hat{\beta_1},\hat{\beta_2})
\end{split}
\end{equation}

```{r task2f}
beta_1 <- coef(m.birds)[2]
beta_2 <- coef(m.birds)[3]
omega <- sqrt(-1/(2*beta_2))
theta <- beta_1/(-2*beta_2)
```

By using the equations and chunk above we have found that $\hat\omega=`r omega`$ and $\hat\theta=`r theta`$.

In order to find the standard error for $\omega$ and $\theta$ we use the delta method to find the variance of the two parameters:

\begin{equation}
\begin{split}
  \text{Var}(\hat\omega) &= (\frac{\partial g}{\partial \hat{\beta_1}})^2\cdot \text{Var}(\hat{\beta_1}) + (\frac{\partial g}{\partial \hat{\beta_2}})^2\cdot \text{Var}(\hat{\beta_2}) + 2(\frac{\partial g^2}{\partial \hat{\beta_1} \partial \hat{\beta_2}})\cdot \text{Cov}(\hat{\beta_1},\hat{\beta_2}) \\
  &= (\frac{\partial g}{\partial \hat{\beta_2}})^2\cdot \text{Var}(\hat{\beta_2}) \\
  &= (\frac{1}{\sqrt{-8\hat{\beta_2^3}}})^2 \cdot \text{Var}(\hat{\beta_2})
\end{split}
\end{equation}


\begin{equation}
\begin{split}
  \text{Var}(\hat\theta) &= (\frac{\partial h}{\partial \hat{\beta_1}})^2\cdot \text{Var}(\hat{\beta_1}) + (\frac{\partial h}{\partial \hat{\beta_2}})^2\cdot \text{Var}(\hat{\beta_2}) + 2(\frac{\partial h}{\partial \hat{\beta_1}})(\frac{\partial h}{\hat\beta_2})\cdot \text{Cov}(\hat{\beta_1},\hat{\beta_2}) \\
  &= (-\frac{1}{2\hat\beta_2})^2 \cdot \text{Var}(\hat\beta_1)+(\frac{\hat\beta_1}{2\hat\beta_2^2})^2\cdot \text{Var}(\hat\beta_2)+2(\frac{-1}{2\hat\beta_2})(\frac{\hat\beta_1}{2\hat\beta_2^2})\cdot \text{Cov}(\hat\beta_1,\hat\beta_2) 
\end{split}
\end{equation}


```{r task2f_2}
# Variance and covariance
var.beta_1 <- m.birds$vcov[2,2]
var.beta_2 <- m.birds$vcov[3,3]
cov.beta_12 <- m.birds$vcov[2:3,2:3]
#gradient of g and h
gradient_g <- c(0,1/sqrt(-8*beta_2^3))
gradient_h <- c(-1/(2*beta_2),beta_1/(2*beta_2^2))
# Computing standard errors
sd_omega <- sqrt(t(gradient_g)%*%cov.beta_12%*%gradient_g)
sd_theta <- sqrt(t(gradient_h)%*%cov.beta_12%*%gradient_h)
```

The standard error for $\omega$ and $\theta$ respectively is `r sd_omega` and `r sd_theta`.


### g)

Based on the fitted model the estimated optimal date is $\theta$, as the expected number of fledglings on time t $\lambda_i(t)$ is gaussian distributed with mean $\theta$. Because of global warming, the actual mean time can change to earlier dates. We will perform a hypothesis test to check if this is the case. We let $\mu$ denote the mean value of $t$, and define the hypothesis as $$H_0: \theta = \mu; H_1: \theta\neq\mu.$$ We must assume that $\theta$ and the mean $\mu$ is approximately gaussian, and that they are independent of each other. As $\mu$ is found from some given values of $t$, and $\theta$ can be seen as a function of $y_i$ s, we can assume this is the case.

```{r task 2g}
ts <- data[,2]
mu <- sum(ts)/length(ts)
z.obs <- (mu - theta)/(omega/sqrt(length(ts)))
p_val <- 2*pnorm(-abs(z.obs))  # symmatry of normal distribution
```

The p-value for this hypothesis test is `r p_val`, which indicates that we can reject the null hypothesis for a significance level of 0.05. That is, the mean value of t in the population we are looking at is different from the estimated optimal time based on the fitted model.

### h)

```{r}
d.boot = data
n = length(d.boot$y)

#matrix for recording betas
beta_boot_matrix = matrix(ncol=3, nrow=1000)

#loop
for (i in 1:1000) {
  #simulating y-s
  y_boot = rpois(n, d.boot$E_y)
  d.boot$y = y_boot
  #refitting model
  m.boot = myglm(y~t+I(t^2), data = d.boot)
  #recording beta
  beta_boot = m.boot$coefficients[,1]
  beta_boot_matrix[i,] = beta_boot
}

#calculating the variance from parametric bootstrapping
var_boot = diag(var(beta_boot_matrix))
#the variance from originally fitted model
var_model = diag(m.birds$vcov)

var_boot
var_model
```

The estimated variance, from simulation, is very similar to what we expect from Expected Fisher Information. So the Expected Fisher Information is a good approximation.

